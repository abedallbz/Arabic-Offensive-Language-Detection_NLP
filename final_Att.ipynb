{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adb64SEY5RMn"
      },
      "source": [
        "# loading arabicWord2vec model\n",
        "from gensim.models  import Doc2Vec,Word2Vec\n",
        "sg_ar_twitter =Word2Vec.load(\"/content/drive/My Drive/OffensEval 2020/Data/embeddings_skip_grams_100/full_grams_sg_100_twitter.mdl\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fODW8pAN47sI"
      },
      "source": [
        "import re\n",
        "import codecs\n",
        "def read_data(file_path):\n",
        "  X = []\n",
        "  Y = []\n",
        "  pos_num=0\n",
        "  neg_num=0\n",
        "  file1= codecs.open(file_path, 'r', 'utf-8')\n",
        "  for line in file1:\n",
        "    if line == \"nan\" or line == \"\\n\":\n",
        "      continue\n",
        "    out = line.split(',')\n",
        "    X.append(out[0])\n",
        "    label=out[1]\n",
        "    if \"OFF\" in label:\n",
        "      Y.append(1)\n",
        "      pos_num+=1\n",
        "    else:\n",
        "      Y.append(0)\n",
        "      neg_num+=1\n",
        "\n",
        "  return X,Y, pos_num, neg_num# # read data test\n",
        "\n",
        "\n",
        "# read data test\n",
        "def read_X_test():\n",
        "  test=codecs.open('/content/drive/My Drive/OffensEval 2020/Data/offensive_eval_subtask_A/OSACT2020-sharedTask-test-tweets.txt','r','utf-8')\n",
        "  X_test=[]\n",
        "  for line in test:\n",
        "      t = normalize_text(line)\n",
        "      X_test.append(t)\n",
        "  test.close()\n",
        "  print(\"test data \" , len(X_test))\n",
        "  return X_test\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpGOQIn_SKBR"
      },
      "source": [
        "\n",
        "\n",
        "x_train, y_train, pos_num, neg_num = read_data('/content/drive/My Drive/OffensEval 2020/Data/offensive_eval_subtask_A/taskA_cleaned_stemmed_train_file1.txt')\n",
        "print(len(y_train))\n",
        "x_val, y_val, pos_num1, neg_num1 = read_data('/content/drive/My Drive/OffensEval 2020/Data/offensive_eval_subtask_A/taskA_cleaned_stemmed_val_file1.txt')\n",
        "print(len(y_val))\n",
        "#x_test = read_X_test()\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOisKxheRn8J"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bkUcLn35Hqe"
      },
      "source": [
        "import numpy as np\n",
        "# retrun embedding matrix [(all word in copus) ,(dimension for every word)]\n",
        "\n",
        "def create_embedding_matrix(model, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    i=0\n",
        "    for word in word_index:\n",
        "      try :\n",
        "        idx = word_index[word]\n",
        "        word_vector = model.wv[word]\n",
        "        embedding_matrix[idx] = np.array(\n",
        "                      word_vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "      except KeyError:\n",
        "        embedding_matrix[idx]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
        "\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlDHGZJV9fs0"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=50000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "\n",
        "x_val = tokenizer.texts_to_sequences(x_val)\n",
        "#x_test = tokenizer.texts_to_sequences(X_test)\n",
        "#X = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "vocab_size = len(tokenizer.index_word) + 1\n",
        "print(vocab_size)\n",
        "\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(sg_ar_twitter, tokenizer.word_index,100)\n",
        "\n",
        "#del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax2m1SYs5b82"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_len = 50\n",
        "x_train = pad_sequences(x_train, maxlen=max_len,truncating='post', padding='post', value = 0)\n",
        "x_val = pad_sequences(x_val, maxlen=max_len,truncating='post', padding='post', value = 0)\n",
        "#x_test =pad_sequences(X_test, maxlen=max_len,truncating='post', padding='post', value = 0)\n",
        "#print(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CpJPb1pARiT"
      },
      "source": [
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.initializers import *\n",
        "from keras.optimizers import *\n",
        "import keras.backend as K\n",
        "from keras.callbacks import *\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EAIfRp2R-DJ"
      },
      "source": [
        "def train(x_train , y_train , x_val, y_val , model_obj):\n",
        "        clf = model_obj\n",
        "        clf.load_weights('model.h5')\n",
        "        history = clf.fit(x_train, y_train, batch_size=100, epochs=20, validation_data=(x_val, y_val) )\n",
        "        # class_weight={1:0.9, 0:0.1}\n",
        "        return history ,clf\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9dVtqoy5b7y"
      },
      "source": [
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "def model_lstm_atten(embedding_matrix):\n",
        "    inp = Input(shape=(max_len,))\n",
        "    print(vocab_size)\n",
        "    x = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x=Dropout(0.5)(x)\n",
        "    # note : we can use CuDNNLSTM rethar than CuDNNGRU\n",
        "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
        "    x=Dropout(0.5)(x)\n",
        "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
        "    x = AttentionWithContext()(x)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam',  metrics=['accuracy',precision_m, recall_m, f1_m])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX5hqlfh5Rcf"
      },
      "source": [
        "model = model_lstm_atten(embedding_matrix)\n",
        "# Changed it here a little bit since the custom attention layer is not getting deepcopy\n",
        "model.save_weights('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHHNGU4y6Ude"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "#x_val=[i[0] for i in x_val]\n",
        "#print(x_train)\n",
        "#print(y_val)\n",
        "hist , model_tr = train(x_train , y_train , x_val , y_val,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Hw_1ja_Vkg"
      },
      "source": [
        "loss, acc, precesion, recall, f1= model.evaluate(x_val, y_val)\n",
        "print(\"Loss: \",loss)\n",
        "print(\"acc: \",acc)\n",
        "print(\"precesion: \",precesion)\n",
        "print(\"recall: \",recall)\n",
        "print(\"f1: \",f1)\n",
        "#y_predict=model.predict(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHDkRhGe9u3m"
      },
      "source": [
        "from tqdm import tqdm_notebook, tnrange\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "def bestThresshold(y_train,train_preds):\n",
        "    tmp = [0,0,0] # idx, cur, max\n",
        "    delta = 0\n",
        "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
        "        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
        "        if tmp[1] > tmp[2]:\n",
        "            delta = tmp[0]\n",
        "            tmp[2] = tmp[1]\n",
        "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
        "    return delta , tmp[2]\n",
        "\n",
        "valid= model.predict(x_val)\n",
        "t=[i[0] for i in valid]\n",
        "delta, _= bestThresshold(y_val,t)\n",
        "'''\n",
        "off = 0 ------> best threshold is 0.2300 with F1 score: 0.9574\n",
        "off = 1 ----- >best threshold is 0.3600 with F1 score: 0.7305\n",
        "---------------------------------------------------------------------\n",
        "Hs = 1 ----- >best threshold is 0.1200 with F1 score: 0.3736\n",
        "Hs = 0 ------>best threshold is 0.2600 with F1 score: 0.9805\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQyV6ksRQFPS"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "def plot_confusion_matrix(y_test,y_pred, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "  \"\"\"\n",
        "  This function prints and plots the confusion matrix.\n",
        "  Normalization can be applied by setting `normalize=True`.\n",
        "  \"\"\"\n",
        "  clf_report = classification_report(y_test, y_pred, target_names=classes, output_dict=True)\n",
        "  # .iloc[:-1, :] to exclude support\n",
        "  sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qEocxX-QLx_"
      },
      "source": [
        "def predict_with_best_threshold(y_predict , threshold):\n",
        "  predict=[]\n",
        "  for i in range(len(y_predict)):\n",
        "    if y_predict[i] >= threshold :\n",
        "      predict.append(1)\n",
        "    else :\n",
        "      predict.append(0)\n",
        "  return predict\n",
        "\n",
        "predicted=model.predict(x_val)\n",
        "\n",
        "plot_confusion_matrix(y_val, predicted.round(), classes=list(set(y_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvNDIHjx3K3M"
      },
      "source": [
        "predicted=model.predict(x_val)\n",
        "pre = predict_with_best_threshold(predicted ,delta)\n",
        "plot_confusion_matrix(y_val, pre, classes=list(set(y_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ozyOk6015SO"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjIVMl_2p1zH"
      },
      "source": [
        "model.save('/content/drive/My Drive/OffensEval 2020/Models/attention_model/model_att2_stemming.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSzy1z2ep1xf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swVck5d4p1v1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeF7W-SF6nS1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLmg3CLo6nQA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-waV1krbTnk"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yee6Q8CDbT5M"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x6jmgbFbT3q"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Sce9ItbT1x"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1huWzBV2bT0A"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUT-PjBAbTmE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkvcDXSUbTkM"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCzfhX-abTia"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYijdSpDbTgA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO8ybjYVbTcv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbh2aNDIp1t_"
      },
      "source": [
        "## don't read down not completed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYvjWwNCIE7z"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import codecs\n",
        "import csv\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk.corpus\n",
        "from nltk.text import Text\n",
        "from sklearn.utils import shuffle\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import utils\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "import nltk\n",
        "# from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
        "from sklearn import preprocessing\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "tknzr = TweetTokenizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peHzeeYaIE5N"
      },
      "source": [
        "import csv\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk.corpus\n",
        "from nltk.text import Text\n",
        "from sklearn.utils import shuffle\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.stem.isri import ISRIStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v43gvOlIIHs"
      },
      "source": [
        "# remove harakat\n",
        "def remove_harakat(in_data):\n",
        "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ّ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    out_data = re.sub(noise, '', in_data)\n",
        "    return out_data\n",
        "\n",
        "\n",
        "# substitute some characters with others\n",
        "def substitute_letters(in_data):\n",
        "    out_data = re.sub(\"[إأٱآا]\", \"ا\", in_data)\n",
        "    out_data = re.sub(\"ى\", \"ي\", out_data)\n",
        "    out_data = re.sub(\"ؤ\", \"ء\", out_data)\n",
        "    out_data = re.sub(\"ئ\", \"ء\", out_data)\n",
        "    out_data = re.sub(\"ة\", \"ه\", out_data)\n",
        "    out_data = re.sub('URL','يورل', out_data) # this will substitute URL with arabic equivilant\n",
        "    return(out_data)\n",
        "\n",
        "# remove everything but characters, spaces and hashtags\n",
        "def remove_symbols(in_data):\n",
        "    pattern=r'\\\\.|[^\\w\\s]'\n",
        "    out_data= re.sub(pattern,' ', in_data)\n",
        "    out_data=re.sub(r'\\d',' ',out_data)\n",
        "    return out_data\n",
        "\n",
        "# remove multiple spaces\n",
        "def remove_spaces(in_data):\n",
        "    pattern=r'\\n|\\r|\\t|(\\s+)'\n",
        "    out_data= re.sub(pattern,' ', in_data)\n",
        "    return out_data\n",
        "\n",
        "# remove common stopwords\n",
        "def remove_stopwords(in_data):\n",
        "    in_data = tknzr.tokenize(in_data)\n",
        "    out_data=[w for w in in_data if not w in stop_words ]\n",
        "    result = \" \"\n",
        "    result = result.join(out_data)\n",
        "    return result\n",
        "\n",
        "# remove duplicate consecutive characters\n",
        "# don't allow more than 2 repetiation for one character at a time\n",
        "def remove_duplicate_characters(in_data):\n",
        "    return re.compile(r\"(.)\\1{2,}\", re.IGNORECASE).sub(\"\\\\1\", in_data)\n",
        "\n",
        "# stemming arabic text using ISRIStemmer\n",
        "def light_stem1(text):\n",
        "  words = text.split()\n",
        "  result = list()\n",
        "  stemmer = ISRIStemmer()\n",
        "  for word in words:\n",
        "    word= stemmer.stem(word)\n",
        "    result.append(word)\n",
        "\n",
        "  return ' '.join(result)\n",
        "\n",
        "def light_stem2(text):\n",
        "  words = text.split()\n",
        "  result = list()\n",
        "  stemmer = ISRIStemmer()\n",
        "  for word in words:\n",
        "    word = stemmer.norm(word, num=1)      # remove diacritics which representing Arabic short vowels\n",
        "    if not word in stemmer.stop_words:    # exclude stop words from being processed\n",
        "      word = stemmer.pre32(word)        # remove length three and length two prefixes in this order\n",
        "      word = stemmer.suf32(word)        # remove length three and length two suffixes in this order\n",
        "      word = stemmer.waw(word)          # remove connective ‘و’ if it precedes a word beginning with ‘و’\n",
        "      word = stemmer.norm(word, num=2)  # normalize initial hamza to bare alif\n",
        "    result.append(word)\n",
        "  return ' '.join(result)\n",
        "\n",
        "def light_stem3(text):\n",
        "  ArListem = ArabicLightStemmer()\n",
        "  words = text.split()\n",
        "  result = list()\n",
        "  for word in words:\n",
        "    word= ArListem.light_stem(word)\n",
        "    result.append(word)\n",
        "\n",
        "  return ' '.join(result)\n",
        "\n",
        "\n",
        "# remove english characters\n",
        "def remove_english_characters(in_data):\n",
        "    pattern=r'[A-Za-z]'\n",
        "    out_data= re.sub(pattern,'', in_data)\n",
        "    return out_data\n",
        "\n",
        "## Misc things\n",
        "import codecs\n",
        "pattern=r'\\r\\n|\\n'\n",
        "stop_words_file=codecs.open('/content/drive/My Drive/OffensEval 2020/Source Code/Arabic- StopWords.txt','r','utf-8')\n",
        "stop_words=[]\n",
        "for line in stop_words_file:\n",
        "    new_line=re.sub(pattern,'',line)\n",
        "    stop_words.append(new_line)\n",
        "stop_words_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fvQbBDtIIo5"
      },
      "source": [
        "# normalize text by using a set of functions\n",
        "def normalize_text(in_data, with_stemming=False):\n",
        "        clean_data=remove_harakat(in_data)# remove harakat\n",
        "        clean_data=remove_symbols(clean_data)# we didn't remove # because we're going to make use of them\n",
        "        clean_data=substitute_letters(clean_data)# substitute some letters with others\n",
        "        clean_data= remove_english_characters(clean_data) # remove english characters\n",
        "\n",
        "        clean_data=remove_stopwords(clean_data)# remove stop words from our text before removing harakat\n",
        "        if with_stemming==True:\n",
        "            clean_data=light_stem2(clean_data)\n",
        "        clean_data=remove_spaces(clean_data) # remove spaces between words\n",
        "        clean_data=remove_duplicate_characters(clean_data) # remove duplicate characters\n",
        "        return clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eJNm57eILnS"
      },
      "source": [
        "import re\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# get  X_train and Y _train and X_test and Y_test\n",
        "'''\n",
        "\n",
        "'''\n",
        "\n",
        "labels = []\n",
        "def read1(file):\n",
        "    X = []\n",
        "    Y = []\n",
        "    with open(file ,encoding='utf-8') as fp:\n",
        "        line = fp.readline()\n",
        "        while line:\n",
        "            if line == \"nan\" or line == \"\\n\":\n",
        "                continue\n",
        "            else :\n",
        "                if \"NOT_HS\" in line:\n",
        "                    # Label 1\n",
        "                    line = normalize_text(line)\n",
        "                    X.append(line)\n",
        "                    Y.append(0)\n",
        "                elif \"HS\" in line:\n",
        "                    line = normalize_text(line)\n",
        "                    X.append(line)\n",
        "                    Y.append(1)\n",
        "            line = fp.readline()\n",
        "    fp.close()\n",
        "    return X,Y\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO8iHi_4IN3C"
      },
      "source": [
        "\n",
        "x_train , y_train = read1(\"/content/drive/My Drive/OffensEval 2020/Data/offensive_eval_subtask_B/training set.txt\" )\n",
        "print(len(x_train))\n",
        "x_val  , y_val = read1(\"/content/drive/My Drive/OffensEval 2020/Data/offensive_eval_subtask_B/Development set.txt\")\n",
        "print(len(x_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KN7ecW0uKT8"
      },
      "source": [
        "import imblearn\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "# down_sampling\n",
        "smote = SMOTE(ratio='minority')\n",
        "x_train, y_train = smt.fit_sample(x_train, y_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsmB5u3O2M29"
      },
      "source": [
        "from imblearn.under_sampling import ClusterCentroids\n",
        "print\n",
        "cc = ClusterCentroids(ratio={0: 10})\n",
        "x_train, y_train = cc.fit_sample(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln_4O0byzoxH"
      },
      "source": [
        "# under_sampling and over_sampling\n",
        "import imblearn\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "smt = SMOTETomek(ratio='auto')\n",
        "x_train, y_train = smt.fit_sample(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk_AQIX9KD6r"
      },
      "source": [
        "count= 0\n",
        "for i in range(len(y_train)):\n",
        "  if y_train[i] == 0:\n",
        "    count+=1\n",
        "print(count)\n",
        "print(len(x_train))\n",
        "print(len(y_train))\n",
        "print(x_train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6hwMwvkvFwQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdrCtAtmvM5E"
      },
      "source": [
        "X_train1, y_train1, pos_num1, neg_num1 = read_data('/content/drive/My Drive/OffensEval 2020/Data/offensive_eval_subtask_A/taskA_cleaned_train_file1.txt')\n",
        "print(len(y_train1), pos_num1, neg_num1, pos_num1/ len(y_train1))\n",
        "x_val, y_val, pos_num_val, neg_num_val =read_data('/content/drive/My Drive/OffensEval 2020/Data/offensive_eval_subtask_A/taskA_cleaned_val_file1.txt')\n",
        "print(len(y_val), pos_num_val, neg_num_val, pos_num_val/len(y_val))\n",
        "X_train2, y_train2, pos_num2, neg_num2 = read_data('/content/drive/My Drive/OffensEval 2020/Data/offensive_eval_subtask_A/taskA_cleaned_train_youtube_file1.txt')\n",
        "print(len(y_train2), pos_num2, neg_num2, pos_num2/len(y_train2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJt_CC6I5g_O"
      },
      "source": [
        "print(len(X_train2))\n",
        "def balance(x_data, y_data, num_neg_removed):\n",
        "  count=0\n",
        "  new_x_data=[]\n",
        "  new_y_data=[]\n",
        "  for x,y in zip(x_data, y_data):\n",
        "    if count>num_neg_removed:\n",
        "      new_x_data.append(x)\n",
        "      new_y_data.append(y)\n",
        "    else:\n",
        "      if y==1:\n",
        "        new_x_data.append(x)\n",
        "        new_y_data.append(y)\n",
        "      else: count+=1\n",
        "\n",
        "  return new_x_data, new_y_data\n",
        "\n",
        "X_train2, y_train2=balance(X_train2, y_train2, 6000)\n",
        "print(len(X_train2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qivimqld5ke8"
      },
      "source": [
        "import random\n",
        "x_train = X_train2 + X_train1\n",
        "y_train = y_train2 + y_train1\n",
        "\n",
        "combined = list(zip(x_train, y_train))\n",
        "random.shuffle(combined)\n",
        "x_train, y_train= zip(*combined)\n",
        "\n",
        "x_train = X_train\n",
        "print(len(x_train))\n",
        "print(y_train)\n",
        "print(len(x_val))\n",
        "y_train = list(y_train)\n",
        "print(y_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p959Kro6Tm3"
      },
      "source": [
        "y_train = list(y_train)\n",
        "print(y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1XscEhp-H-m"
      },
      "source": [
        "print(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TuJUcTX-LAQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}